# -*- coding: utf-8 -*-
"""Copy of Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kk9cf-u9pc7kTviNWSa3YKNFx7rDQDHq
"""

# Import the pandas library to read our dataset
import pandas as pd
# Get the train/test split package from sklearn for preparing our dataset to
# train and test the model with
from sklearn.model_selection import train_test_split
# Import the numpy library to work with and manipulate the data
import numpy as np

# Reads a comma-separated value (CSV) file
dataset = pd.read_csv('/content/sample_data/housing.csv') # The path to your .csv file
# Remove any incomplete entries
dataset = dataset.dropna()
# View the first five rows of the dataset
print("Here are the first ten rows of the dataset:")
dataset.head(10)

import matplotlib.pyplot as plt

fig, ax = plt.subplots(9, sharex='col', sharey='row')

ax[0].plot(dataset['longitude'][0:18],color='red',label="longitude")
ax[0].legend()
ax[1].plot(dataset['latitude'][0:18],color='blue',label="latitude")
ax[1].legend()
ax[2].plot(dataset['housing_median_age'][0:18],color='black',label="housing_median_age")
ax[2].legend()
ax[3].plot(dataset['total_rooms'][0:18],color='violet',label="total_rooms")
ax[3].legend()
ax[4].plot(dataset['total_bedrooms'][0:18],color='orange',label="total_bedrooms")
ax[4].legend()
ax[5].plot(dataset['population'][0:18],color='green',label="population")
ax[5].legend()
ax[6].plot(dataset['households'][0:18],color='cyan',label="households")
ax[6].legend()
ax[7].plot(dataset['median_income'][0:18],color='indigo',label="median_income")
ax[7].legend()
ax[8].plot(dataset['median_house_value'][0:18],color='yellow',label="median_house_value")
ax[8].legend()


plt.show()

# We will predict the "median_house_value" column
Y = dataset['median_house_value'] # The remainder of the columns will be used to predict Y # Select from the "longitude" column to the "median_income" column
X = dataset.loc[:,'longitude':'median_income']

# Splits the dataset so 70% is used for training and 30% for testing
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=2003) # Converts the datasets to numpy arrays to work with our PyTorch model
x_train_np = x_train.to_numpy()
y_train_np = y_train.to_numpy()
# Convert the testing data
x_test_np = x_test.to_numpy()
y_test_np = y_test.to_numpy()

# Import the pytorch library
import torch
# Import the 1D convolution layer
# Since we’re inputting a 1-dimensional row of data, we can’t use 2D or 3D
from torch.nn import Conv1d
# Import the max pooling layer
from torch.nn import MaxPool1d
# Import the flatten layer
from torch.nn import Flatten
# Import the linear layer
from torch.nn import Linear
# Import the ReLU activation function
from torch.nn.functional import relu
# Import the DataLoader and TensorDataset libraries from PyTorch
# to work with our datasets
from torch.utils.data import DataLoader, TensorDataset

# Our class MUST be a subclass of torch.nn.Module
class CnnRegressor(torch.nn.Module):
   # Define the initialization method
  def __init__(self, batch_size, inputs, outputs):
    super(CnnRegressor, self).__init__()
    self.batch_size = batch_size
    self.inputs = inputs
    self.outputs =outputs
    # Define the input layer
    # (input channels, output channels, kernel size)
    self.input_layer = Conv1d(inputs, batch_size, 1)
    self.max_pooling_layer = MaxPool1d(1) # Constructor value:  Kernel size = 1
    self.conv_layer = Conv1d(batch_size, 128, 1)
    self.conv_layer2 = Conv1d(128, 64, 1)
    self.flatten_layer = Flatten()
    self.liner_layer = Linear(64, 32)
    self.output_layer = Linear(32, outputs)
    # Define a method to feed inputs through the model
  def feed(self, input):
    input = input.reshape((self.batch_size, self.inputs, 1))  #single dimension for tabular data (expects 3D data(image))
    output = relu(self.input_layer(input))
    output = self.max_pooling_layer(output)
    output = relu(self.conv_layer(output))
    output = relu(self.conv_layer2(output))
    output = self.flatten_layer(output)
    output = self.liner_layer(output)
    output = self.output_layer(output)
    return output

# Import the SGD (stochastic gradient descent) package from pytorch for
# our optimizer
from torch.optim import SGD
from torch.optim import Adam
# Import the L1Loss (mean absolute error loss) package from pytorch for
# our performance measure
from torch.nn import L1Loss
# Import the R^2 score package from pytorch's ignite for our score measure
# This package is not installed by default so the next line does that
!pip install pytorch-ignite
from ignite.contrib.metrics.regression.r2_score import R2Score

# Define the batch size we'd like to use
batch_size = 64
# (batch size, X columns, Y columns)
model = CnnRegressor(batch_size, X.shape[1], 1) # Set the model to use the GPU for processing
model.cuda()

# This method will return the average L1 loss and R^2 score
# of the passed model on the passed DataLoader
def model_loss(model, dataset, train = False, optimizer = None):
  # Cycle through the batches and get the average L1 loss
  performance = L1Loss()
  score_metric = R2Score()
  avg_loss = 0
  avg_score = 0
  count = 0
  for input, output in iter(dataset):
    # Get the model's predictions for the training dataset
    predictions = model.feed(input) # Get the model's loss
    loss = performance(predictions, output)
    # Get the model's R^2 score
    score_metric.update([predictions, output])
    score = score_metric.compute()
    if(train):
      # Clear any errors so they don't cummulate
      optimizer.zero_grad()
      # Compute the gradients for our optimizer
      loss.backward()
      # Use the optimizer to update the model's parameters based on the gradients
      optimizer.step()
    # Store the loss and update the counter
    avg_loss += loss.item()
    avg_score += score
    count += 1
  return avg_loss / count, avg_score / count

import time
# Define the number of epochs to train for
epochs = 500
learning_rate = 0.002
# Define the performance measure and optimizer
optimizer = Adam(model.parameters(), learning_rate) # Convert the training set into torch variables for our model using the GPU
# as floats. The reshape is to remove a warning pytorch outputs otherwise.
inputs = torch.from_numpy(x_train_np).cuda().float()
outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0], 1)).cuda().float()
# Create a DataLoader instance to work with our batches
tensor = TensorDataset(inputs, outputs)
loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True) # Start the training loop
start_time = time.time()
for epoch in range(epochs):
  # Cycle through the batches and get the average loss
  avg_loss, avg_r2_score = model_loss(model, loader, train=True, optimizer=optimizer)
  # Output the average loss
  print("Epoch " + str(epoch + 1) + ":\n\tLoss = " + str(avg_loss) + "\n\tR^2 Score = " + str(avg_r2_score))

filepath = "/content/sample_data/1114334"
torch.save(model,filepath)
model = torch.load(filepath)

# Convert the testing set into torch variables for our model using the GPU
# as floats
inputs = torch.from_numpy(x_test_np).cuda().float()
outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0], 1)).cuda().float()
# Create a DataLoader instance to work with our batches
tensor = TensorDataset(inputs, outputs)
loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True) # Output the average performance of the model
end_time = time.time()
print("Inference time: "+str(end_time- start_time))
avg_loss, avg_r2_score = model_loss(model, loader)
print("The model's L1 loss is: " + str(avg_loss))
print("The model's R^2 score is: " + str(avg_r2_score))

